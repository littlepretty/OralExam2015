\section{Implementation}
\label{Sec-Implementation}

The implementation of the virtual time system and its integration with Mininet-Hifi (the latest version of Mininet) is composed of three parts, as shown in Figure \ref{Fig-VT-Mininet-Hifi}. First, we built a lightweight and independent middleware in the Linux kernel to provide virtual time support to user-space software. Second, we slightly modified the initialization procedure of Mininet with two additional python modules to realize (adaptive) virtual time in Mininet. Third, we discuss our design to enable transparent support of virtual time for applications running in the containers.

\begin{figure*}
\centering
\epsfig{file=figures/VT-Mininet-Hifi.eps, height=3.3in, width=5.5in}
\caption{\textbf{Integration of Mininet-Hifi and Virtual Time}}
\label{Fig-VT-Mininet-Hifi}
\end{figure*}

\subsection{Linux Kernel Modifications}
\label{Sub-Sec-ExtendLinuxKernel}

Our implementation is based on a recent Linux kernel 3.16.3 with no third-part library dependency.

\subsubsection{Timing-related Kernel Modifications}
To make a process have its own perception of time, we added the following four new fields in the \texttt{task\_struct} struct type.
\begin{itemize}
	\item \texttt{virtual\_start\_nsec} represents the starting time that a process detaches from the system clock and uses the virtual time, in nanoseconds 
	\item \texttt{physical\_past\_nsec} represents how much physical time has elapsed since the last time the process requested the current time, in nanoseconds 
	\item \texttt{virtual\_past\_nsec} represents how much virtual time has elapsed since the last time the process requested the current time, in nanoseconds
	\item \texttt{dilation} represents the TDF of a time-dilated process
\end{itemize}

\setlength{\textfloatsep}{0pt}% Remove \textfloatsep
\setlength{\intextsep}{0pt}
\setlength{\floatsep}{0pt}
\algrenewcommand{\algorithmiccomment}[1]{\hskip3em$/*$ #1 $*/$}
\begin{algorithm*}[t]
\caption{Time Dilation Algorithm}%: Dilate \texttt{ts} if a current process uses virtual clock}
\label{Alg-DilateTimeKeeping}
\begin{algorithmic}[1]
\Function{init\_virtual\_time}{\texttt{struct task\_struct *tk, int dilation}}
\If{\texttt{dilation>0}}
    \State \texttt{tk$\rightarrow$dilation=dilation}
    \State \texttt{tk$\rightarrow$virtual\_start\_nsec=0}
    \State \texttt{struct timespec ts}
    \State \texttt{getnstimeofday(\&ts)}
    \State \texttt{tk$\rightarrow$virtual\_start\_nsec=timespec\_to\_ns(\&ts)}
    \State \texttt{tk$\rightarrow$physical\_past\_nsec=0}
    \State \texttt{tk$\rightarrow$virtual\_past\_nsec=0}
\EndIf
\EndFunction
\\
\Function{do\_dilatetimeofday}{\texttt{struct timespec *ts}}
\State{Let \texttt{p} denote the current process using virtual time} %system-wide}
\If{\texttt{p$\rightarrow$virtual\_start\_nsec > 0  and p$\rightarrow$dilation > 0}}
	\State \texttt{now = timespec\_to\_ns(ts)}
	\State \texttt{physical\_past\_nsec = now - p$\rightarrow$virtual\_start\_nsec}
	\State \texttt{virtual\_past\_nsec = (physical\_past\_nsec - p$\rightarrow$physical\_past\_nsec) / p$\rightarrow$dilation + p$\rightarrow$virtual\_past\_nsec}\Comment{virtual time computation}
	\State \texttt{dilated\_now = virtual\_past\_nsec + p$\rightarrow$virtual\_start\_nsec}
	\State \texttt{dilated\_ts = ns\_to\_timespec(dilated\_now)}
	\State \texttt{ts$\rightarrow$tv\_sec = dilated\_ts.tv\_sec}
	\State \texttt{ts$\rightarrow$tv\_nsec = dilated\_ts.tv\_nsec}
	\State \texttt{p$\rightarrow$physical\_past\_nsec = physical\_past\_nsec}\Comment{update process's physical time}
	\State \texttt{p$\rightarrow$virtual\_past\_nsec = virtual\_past\_nsec}\Comment{update process's virtual time}
\EndIf
\EndFunction
\end{algorithmic}
\end{algorithm*}
Algorithm \ref{Alg-DilateTimeKeeping} gives the details about how we implement the time dilation. To preserve an accurate virtual clock in the kernel, we added a private function \texttt{do\_dilatetimeofday} in the Linux's timekeeping subsystem to keep tracking the dilated virtual time based on the physical time passed and TDF. Based on process's \texttt{virtual\_start\_nsec}, the system determines the type of time to return, i.e., the physical system clock time or the virtual time.

\texttt{virtual\_start\_nsec} in \texttt{INIT\_VIRTUAL\_TIME} should first be initialized to zero so that the next gettimeofday always returns the undilated time to compute and record the exact physical time that a process starts to use virtual time.
To return the accurate virtual time upon requests, the duration since the last call to \texttt{do\_dilatetimeofday} is calculated and precisely scaled with TDF. To enable virtual time support for a wide range of timing-related system calls, we extensively traced the routines in Linux's subsystems that request timing information (such as \texttt{getnstimeofday}, \texttt{ktime\_get}, \texttt{ktime\_get\_ts}, etc.), and modified them to properly invoke \texttt{do\_dilatetimeofday}.


\subsubsection{Process-related Kernel Modifications}
To enable the virtual time perception to processes running in a network emulator, we added the following new system calls.
\begin{itemize}
\item \texttt{virtualtimeunshare} is essentially the \texttt{unshare} system call with time dilation inputs. It is used by container-based emulators, such as Mininet, to create emulated nodes. \texttt{virtualtimeunshare} creates a new process with a TDF in a different namespace of its parent process according to \texttt{flags}.
\item \texttt{settimedilaitonfactor} offers an interface to change the TDF of a process. Note that a command executed in an emulated host is equivalent to a shell command executed by \texttt{bash}. Therefore, adjusting a process' TDF requires the change of the calling process' parent (e.g., host's \texttt{bash}), which occasionally would lead to tracing back to the root of the process tree, especially in the case of dynamic TDF adjustment. 
\end{itemize}

We also modified the \texttt{do\_fork} system call to initialize the virtual-time-related attributes of a process, such as using the variable \texttt{stack\_size} to pass the TDF value. Functions in \texttt{timekeeping.c} were modified to invoke \texttt{do\_dilatetimeofday} in order to return the virtual time to system calls like \texttt{gettimeofday} and other kernel routines that request timing information.


\subsubsection{Networking-related Kernel Modifications}
In this work, we focus on capturing all the related system calls and kernel routings to support virtual time in Linux container with the application of Mininet. One particular case related to Mininet is the usage of \texttt{tc}, a network quality-of-service control module in Linux\cite{TrafficControl}. For instance, we can use \texttt{tc} to rate-limit a link to 100 Mbps using Hierarchic Token Bucket (HTB) \texttt{qdisc} in Mininet. If the TDF is set to 8, the link bandwidth would be approximately 800 Mbps from the emulated hosts' viewpoints as we observed from the time-dilated \texttt{iperf3} application.

As a network module in Linux, \texttt{tc} does not reference Linux kernel's time as the way user application does. Therefore, \texttt{tc} is transparent to our virtual time system. One way to solve this problem is to modify the network scheduling code in kernel to provide \texttt{tc} with a dilated traffic rate. In the earlier example with TDF set to 8, the experiment will run 8 times slower than the real time, and we can configure \texttt{tc}'s rate limit as $rate/TDF=12.5$ Mbps to emulate a 100 Mbps link. 
Note that we only tailored HTB in \texttt{tc}, which is the default \texttt{qdisc} used by Mininet. We will generalize the mechanism to other \texttt{qdiscs} including HFSC (Hierarchical Fair Service Curve) and TBF (Token Bucket Filter) in the future.

\subsection{Network Emulator Modifications}
\label{Sub-Sec-ImplementMininet}

\subsubsection{Virtual-Time-Enabled Container}
%Mininet uses Linux's container \cite{LXC} to enable scalable network emulation. 
Containers allow groups of process running on the same kernel to have independent views of system resources, such as process IDs, file systems and network interfaces. We add the virtual time property to a container's \texttt{namespace}\cite{LinuxNamespace} so that every container can have its own virtual clock. We design our system in the way that minimal modifications of Mininet are needed for integration, so that the virtual time system can be easily extended to other Linux-container-based applications. 

We modified the initialization procedure of Mininet, in particular, the \texttt{mnexec} program in Mininet, to process two additional parameters. When we create \texttt{Node}s in Mininet (hosts, switches, and controllers are all inherited from \texttt{Node}), users can feed in a TDF argument with \texttt{virtualtimeunshare} (as a replacement of \texttt{unshare}) with \texttt{-n} option. This way, a system-wide TDF can be conveniently set for all the emulated hosts. We also provide the ability to dynamically adjust the TDF for every emulated host during runtime. To do that, we added a new option \texttt{-t} in \texttt{mnexec} to invoke the aforementioned system call \texttt{settimedilaitonfactor} to do the actual TDF adjustment. The two modifications enable the integration of virtual time in Mininet, and also serve as the basis of the adaptive TDF management.

\subsubsection{Adaptive TDF Scheduling}
To optimize the performance of the virtual-time-enabled Mininet, we developed an adaptive TDF scheduler through two python modules \texttt{MininetMonitor} and \texttt{DilationAdaptor} (refer to Emulation Monitor and Time Dilation Adaptor in Figure \ref{Fig-ContainerVirtualTime}) to accelerate the experiment speed while preserving high fidelity.

\texttt{MininetMonitor} is responsible to monitor the CPU usage of the entire emulation system, consisting of a group of processes including the Mininet emulator, the Open vSwitch module, and emulated nodes (e.g., SDN controllers, hosts and switches). Also, applications are dynamically created, executed and destroyed within containers, in the form of child processes of their parent containers. \texttt{MininetMonitor} utilizes the \texttt{ps} command to collect the group's aggregate CPU percentage and periodically computes and passes the average CPU load statistics to \texttt{DilationAdaptor}. The core of \texttt{DilationAdaptor} is an adaptive algorithm to calculate an appropriate \textit{TDF}. %Only after receiving a new \textit{TDF} can Mininet pause the experiment and update its global time dilation factor. Otherwise it enters the next Epoch directly. 
Global \textit{TDF} updating was achieved by invoking the \texttt{mnexec\ -t\ tdf} program for every running host. 


% Algorithm \ref{Alg-AdaptiveTDF} illustrates our adaptive TDF scheduling, and we implemented it as a threshold-based \texttt{DilationAdaptor}. 
% %From the input fed by \texttt{MininetMonitor}, \texttt{DilationAdaptor} first define current load status. The key idea here is: if emulator is overloaded, we increase TDF with a large value; if it is underloaded, we may consider decrease TDF with a large value. 

% \algnewcommand\algorithmicswitch{\textbf{switch}}
% \algnewcommand\algorithmiccase{\textbf{case}}
% \algnewcommand\algorithmicassert{\texttt{assert}}
% \algnewcommand\Assert[1]{\State \algorithmicassert(#1)}%
% % New "environments"
% \algdef{SE}[SWITCH]{Switch}{EndSwitch}[1]{\algorithmicswitch\ #1\ \algorithmicdo}{\algorithmicend\ \algorithmicswitch}%
% \algdef{SE}[CASE]{Case}{EndCase}[1]{\algorithmiccase\ #1}{\algorithmicend\ \algorithmiccase}%
% \algtext*{EndSwitch}%
% \algtext*{EndCase}%

% \algrenewcommand{\algorithmiccomment}[1]{\hskip1em$/*$ #1 $*/$}
% \begin{algorithm}
% \caption{Adaptive Time Dilation Factor Scheduling}\label{Alg-AdaptiveTDF}
% \begin{algorithmic}[1]
% \State{Decide emulation $state$ through the resource indicator}\Comment{e.g., CPU utilization}
% \State{$MAX$ denotes the maximum allowed TDF value.}
% \State{$\Delta_{large}, \Delta_{small}$ denote the customizable large and small adjustment}
% \Switch{$state$}
% 	\Case{$OVERLOADED$}
% 		\State{$TDF_{new}=TDF_{prev}+\Delta_{large}$}
% 	\EndCase
% 	\Case{$WARNING$}
% 		\State{$TDF_{new}=TDF_{prev}+\Delta_{small}*2$}
% 	\EndCase
% 	\Case{$MODERATE$}
% 		\If{$TDF_{prev} == MAX$}
% 			\State{$TDF_{new}=TDF_{prev}-\Delta_{large}$}
% 		\ElsIf{$TDF_{prev} > \Delta_{small}$}
% 			\State{$TDF_{new}=TDF_{prev}-\Delta_{small}$}
% 		\Else
% 			\State{$TDF_{new}=TDF_{prev}$}%\Comment{Do not change TDF}
% 		\EndIf
% 	\EndCase
% 	\Case{$LIGHT$}
% 		\If{$TDF_{prev} > \Delta_{large}*2$}
% 			\State{$TDF_{new}=TDF_{prev}-\Delta_{large}$}
% 		\Else
% 			\State{$TDF_{new}=TDF_{prev}-\Delta_{small}*2$}
% 		\EndIf
% 	\EndCase
% \EndSwitch
% %\State{return $TDF_{new}$}
% \end{algorithmic}
% \end{algorithm}

Our adaptive virtual time scheduling design is similar to the one used in \cite{NtwkEmultAdaptVirtTime} in spirit with two major differences. First, both techniques target on different platforms. Our technique is applied to Linux-container-based network emulation to support scalable SDN experiments, and theirs uses virtual routing and executes OpenVZ instances inside Xen. Second, their solution needs be deployed on a cluster to emulate a medium-scale network, which results in much higher communication overhead in two types: (1) every VM's monitor needs to report the CPU usage, and (2) the adaptor needs to send new TDF to every VM. Therefore, the message transmission delay in LAN and the processing delay in protocol stacks contributes to the overall communication delay. In contrast, \texttt{MininetMonitor} runs as a lightweight background thread in Mininet, and \texttt{DilationAdaptor} is simply a python object that Mininet has a reference to. The communication in our system is through synchronized queues and method invocations, which is much faster. 

\subsection{Virtual Time Support for Applications in Containers}
\label{Sub-Sec-ModificationApplications}
Network applications running inside containers (e.g., \texttt{iperf3}\cite{iperf3} or \texttt{ping}) should also use virtual time. We do not need to modify the application code because they are running as child processes of Mininet's hosts. A child process always copies its parent's \texttt{task\_struct} when it is forked including the same \texttt{dilation} and \texttt{virtual\_start\_nsec} values. Although \texttt{virtual\_start\_nsec} does not present the virtual starting time of the child process, our algorithm is designed to work with relative values. When applications inquire about the system time, the modified kernel knows that they are using virtual clock and return the virtual clock time instead of the system clock time.

One issue we notice is that the 64-bit Linux kernel running on Intel-based architectures provides the Virtual Dynamic Shared Object (vDSO) and the \texttt{vsyscall} mechanism to reduce the overhead of context switches caused by the frequently invoked system calls, for example, \texttt{gettimeofday} and \texttt{time}\cite{VDSO}. Therefore, applications may bypass our virtual-time-based system calls unless we explicitly use the \texttt{syscall} function. Our solution is to disable vDSO with respect to \texttt{\_\_vdso\_gettimeofday} in order to transparently offer virtual time to applications in the containers.
