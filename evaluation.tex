\section{Evaluation}
\label{Sec-Experiments}

In this section, we give experimental results to validate our implementation of the virtual time supported Mininet-Hifi network emulator. Very straightforward but nontrivial network typologies are adopted to demonstrate that the emulation system described in \ref{Sec-Implementation} can improve fidelity, scalability as well as efficiency. 

All the experiments are conducted on a Dell XPS 8700 Desktop with one Intel Core i7-4790 CPU, 12 GB RAM and one gigabit Ethernet interface. The machine runs a 64-bit Ubuntu 14.04.1 LTS with our customized 3.16.3 Linux kernel. The benchmark scores of this machine's CPU and FPU are: 1.52 seconds for Blowfish, 1045.62 MiB/seconds for CryptoHash, 0.63 seconds for FFT and 2.56 seconds for Raytracing. Our virtual-time-enabled Mininet was built on the latest version of Mininet (2.1.0), also named Mininet-Hifi, at the time of development.

\paragraphb{Fidelity.}
We first evaluate how our virtual time system improves Mininet's fidelity through a basic network scenario:  a single TCP flow transmission through a chain of switches in an emulated SDN network. As shown in Figure \ref{Fig-ChainTopoExample}, the network topology consists of a client-server pair connected by a chain of Open vSwitch switches in Mininet. We setup the default OpenFlow controller to function as a learning switch. In this set of experiments, we connected two hosts through 40 switches in Mininet, and all the links are configured with 10 $\mu s$ delay. We used \texttt{iperf3}\cite{iperf3} to generate a TCP flow between the client and the server. TDF was set to 1 (i.e., no virtual time) and 4 for comparison. We also setup a real testbed for ``ground truth" throughput data collection. The testbed was composed of two machines connected by a 10 Gbps Ethernet link. We varied the bandwidth link from 100 Mbps to 10 Gbps and measured the throughput using \texttt{iperf3}. In the real testbed, we manually configured the link bandwidth and delay using \texttt{tc}, and the delay was set as the corresponding round trip times (RTTs) measured in the switch chain topology in Mininet, so that the networking settings were tightly coupled for comparison. Although we did not setup an exact network with SDN switches, the stabilized TCP throughputs generated by the physical testbed should reflect what occurs in a real SDN network. Each experiment was repeated 10 times and the results with bandwidth 4 Gbps, 8 Gbps and 10 Gbps were reported in Figure \ref{Fig-Perf40SwDiffBw}. 


We observe that when the bandwidth was no greater than 4 Gbps (we only displayed the 4 Gbps case in the figure),  Mininet was able to accurately emulate the TCP flow with and without virtual time, as the average throughputs were very close to the ones collected from the physical testbed. However, when we continued to increase the bandwidth, Mininet was not able to produce the desired throughputs, e.g., 28\% (8 Gbps) and 39\% (10 Gbps) smaller than the physical testbed throughputs. With virtual time (TDF = 4), Mininet was capable to accurately emulate the TCP flow even at high bandwidth, and the results were nearly the same as the ones measured in the physical testbed. 

The root cause is that the machine running Mininet does not have sufficient resources to emulate networks with bandwidth greater than 4 Gbps, which would lead to fidelity issues, e.g., low expected throughputs.
% and higher throughput in the 8 Gbps case than the 10 Gbps case. By examining the TCP flows, we observed that in the 10 Gbps case, the congestion window (cwnd) never reached 4 MB because of multiple re-transmissions (around 450 times in 1 seconds) caused by packet losses and long latency, whereas in the 8 Gbps case, cwnd could reach 6.75 MB. 
Note that we only emulated a single flow, and the results could be much worse and unpredictable in complicated multi-flow scenarios. Results show that virtual time can significantly enhance the performance fidelity by ``slowing down" the experiments so that the system has sufficient resources and time to correctly process the packets. We further illustrate the effect by plotting the time series of throughput changes for the 10 Gbps cases in Figure \ref{Fig-Perf40Sw10GbLink}. With virtual time, the throughputs measured in Mininet closely match the real testbed results; without virtual time ($TDF = 1$), the ramp up speed was much slower, in particular, 22 seconds ($TDF = 1$) rather than 4 seconds ($TDF = 4$), and the throughput was incorrectly stabilized below 6.1 Gbps.

\paragraphb{Scalability.}
Virtual time also improves the scale of networks that one can emulate without losing fidelity. In this set of experiments, we used the same switch chain topology in Figure \ref{Fig-ChainTopoExample}, and set the link bandwidth to 4 Gbps. We want to investigate, with virtual time, how many switches Mininet is capable to emulate without losing fidelity, i.e., preserving nearly 4 Gbps throughput. This time we increased the number of switches with the following values \texttt{\{20, 40, 60, 80, 100\}}, and TDF was selected to be 1 (no virtual time) and 4. We ran \texttt{iperf3} for 25 seconds between the client and the server. Each experiment was repeated ten times, and the throughput measurement is reported in Figure \ref{Fig-ScaleDiffSw4GbLink}.

In the case of $TDF = 1$, the average throughput kept decreasing as the number of switches grew over 20. The throughput decreased dramatically when the number of switches was greater than 60 (e.g., decreased by 41\% for 60 switches, 65\% for 80 switches, and 83\% for 100 switches). The standard deviation, indicated the undesired high disturbance, also grew as number of switches increased. When virtual time was applied with $TDF = 4$, the throughput was always around 3.8 Gbps with small standard derivations in all the experiments. It is clear that virtual time helps to scale up the emulation. In this case, Mininet can emulate 100 switches with 4 Gbps links and still generate the accurate throughputs, rather than being saturated at 20 switches without virtual time. 

We also recorded the running time in Figure \ref{Fig-ScaleTime}. Longer execution time is the tradeoff for the fidelity and scalability improvement. When $TDF=4$, the execution time was about 4 times longer than the time required in the case of $TDF=1$ in all the experiments. In fact, we have conducted extensive experiments with different TDF values on multiple network scenarios. The general observation is that a larger TDF allows an emulator to conduct accurate experiments with larger scale on the same physical machine, but typically requires longer execution time, approximately proportional to the TDF.  This leads to the question on how to balance the speed and fidelity, and our approach is to explore the adaptive time dilation scheduling, whose evaluation is presented in the next section. 

\begin{figure*}
\centering
	\subfloat[\textbf{Switch Chain Topology for Fidelity and Scalability Evaluation}]
	{
		\epsfig{file=figures/TopoChainExample.eps, height=0.5in, width=2.6in}
		\label{Fig-ChainTopoExample}
	}~
	\subfloat[\textbf{Network Topology for Adaptive Virtual Time Evaluation}]
	{
		\epsfig{file=figures/TopoLinearExample.eps, height=1.0in, width=2.7in}
		\label{Fig-LinearTopoExample}
	}
	\caption{\textbf{Network Topologies for Evaluation}}
\end{figure*}

\begin{figure*}
\centering
	\subfloat[\textbf{TCP Throughput with Different Link Bandwidth}]
	{
		\epsfig{file=figures/Perf40SwDiffBw.eps, height=2.0in, width=3.4in}
		\label{Fig-Perf40SwDiffBw}
	}~
	\subfloat[\textbf{TCP Throughput with 10 Gbps Links}]
	{
		\epsfig{file=figures/Perf40Sw10GbLink.eps, height=1.8in, width=3.4in}
		\label{Fig-Perf40Sw10GbLink}
	}

\caption{\textbf{Fidelity Evaluation Experimental Results}}
\end{figure*}

\begin{figure*}
\centering
	%\subfloat[Mean \& Stdev of TCP Throughput.]{
	\subfloat[\textbf{TCP Throughput}]{
		\epsfig{file=figures/ScaleDiff100Sw4GbLink.eps, height=2.0in, width=3.5in}
		\label{Fig-ScaleDiffSw4GbLink}
	}~
%	\subfloat[Scalability Limit Under Different TDFs.]
%	{	
%		\epsfig{file=figures/ScaleSwThreshold.eps, height=2.0in, width=3.5in}
%		\label{Fig-ScaleSwThreshold}
%	}\\
	%\subfloat[Emulation Running Time.]
	\subfloat[\textbf{Emulation Running Time}]
	{
		\epsfig{file=figures/ScaleTime100Sw4GbLink.eps, height=2.0in, width=3.5in}
		\label{Fig-ScaleTime}
	}%
\caption{\textbf{Scalability Evaluation Experimental Results}}
\end{figure*}

\paragraphb{Adaptive TDF Scheduling.}
We design a set of emulation experiments consisting of multiple TCP flows to evaluate the adaptive TDF scheduling algorithm. The network topology has a simple linear structure as shown in Figure \ref{Fig-LinearTopoExample} and consists of 100 hosts and 99 switches. All the links are of 100 Mbps bandwidth and 1 ms delay. We selected 5 non-overlap client-server pairs: \texttt{(h1, h20), (h21, h40), h(41, h60), h(61, h80), (h81, h100)}. The entire experiment was divided into three phases: (1) initially, transmit flow \texttt{(h1,h20)}, (2) after 50 seconds, transmit all five flows, and (3) after 150 seconds, stop all the transmissions except for flow \texttt{(h1,h20)}. The goal is to evaluate how our adaptive time dilation scheduler behaves under dynamic emulation workloads with the peak load exceeding Mininet's capability.
 
We ran the experiments in three cases. In case 1, TDF was set to 1 (i.e., no virtual time) and the adaptive virtual time scheduling was disabled. All flows' TCP throughputs measured by \texttt{iperf3} over time are plotted in Figure \ref{Fig-5FlowsNoVT}. 
%We also record the average CPU utilization of the emulation experiment, obtained by \texttt{MininetMonitor} (see Section \ref{Sub-Sec-ImplementMininet}), in Table \ref{Tab-CompareRunTime}. 
In case 2, we enabled the adaptive time dilation management system with TDF initially set to 1, and conducted the same emulation experiments. Figure \ref{Fig-5FlowsAdaptiveVT} plots the throughputs of all five flows. In case 3, we used a fixed TDF ($TDF = 11$) and disabled the adaptive virtual time scheduling. Results are shown in Figure \ref{Fig-5FlowsFixedVT}. We set $TDF=11$ because 11 was the largest value observed in the TDF changing history in case 2. In addition, the entire trace of the dynamic TDF in case 2 is plotted in Figure \ref{Fig-HistoryTDF}. We repeated each experiment for 5 times and observed very similar behaviors. All the time series reported in Figure \ref{Fig-Adaptive} were based on the data collected from one run. 

In phase 1, Mininet had sufficient system resources to emulate a single TCP flow \texttt{(h1, h21)}. Therefore, we observe the close-to-line-rate throughput, i.e., 100 Mbps, in all three cases. In phase 2, there were five concurrent flows in the network and each case demonstrated different behaviors. Note that those flows were non-overlap flows because they did not share any links or switches. Therefore, all five flows should achieve close-to-line-rate throughputs, i.e., 100 Mbps, in physical world applications.
In case 1, the throughputs of all five flows were very unstable as shown in Figure \ref{Fig-5FlowsNoVT}, which reflected the heavy resource contention in Mininet. In contrast, in case 3, all five flows have stable, close-to-100-Mbps throughputs because of the virtual time. In case 2, we observed disturbances in throughput at the beginning of phase 2, but the five flows quickly converged to the stable close-to-line-rate throughput because the adaptive TDF scheduler managed to compute the optimal TDF value. The details of TDF adjustment are depicted in Figure \ref{Fig-HistoryTDF}. In phase 3, the emulation returned back to a single flow \texttt{(h1, h21)}, and the measured throughputs were accurate in all three cases.
As indicated by Figure \ref{Fig-HistoryTDF}, our scheduler decreased the TDF value accordingly in case 2 to save emulation time in phase 3 while still preserving the fidelity. 

Table \ref{Tab-CompareRunTime} summarizes the execution time, the average TDF, and the rate of execution time in wall clock to the emulation time (200 seconds) of all three cases. 
%Note that in case 1 with no virtual time, the experiment lasted for 240 seconds. The overhead was not introduced by our scheduler nor by Mininet. //but emulation time != wall-clock time. 
We can see that case 3 ($TDF = 11$) is around 10 times slower than case 1 ($TDF = 1$) in order to guarantee fidelity. Our adaptive time dilation scheduler managed to reduce 46\% of the running time as compared to case 3 with little fidelity loss. 

\begin{figure*}
\centering
	\subfloat[\textbf{TCP Throughput without Virtual Time}]
	{
		{\epsfig{file=figures/5FlowsNoVT.eps, height=1.8in, width=3.3in}}
		\label{Fig-5FlowsNoVT}
	}~
	\subfloat[\textbf{TCP Throughput with Adaptive Time Dilation}]
	{
		\epsfig{file=figures/5FlowsAdaptiveVT.eps, height=1.8in, width=3.3in}
		\label{Fig-5FlowsAdaptiveVT}
	}
	
	\subfloat[\textbf{TCP Throughput with Fixed Time Dilation (11)}]
	{
		\epsfig{file=figures/5FlowsFixedVT.eps, height=1.8in, width=3.3in}
		\label{Fig-5FlowsFixedVT}
	}~
	\subfloat[\textbf{TDF Trace: Adaptive Virtual Time Scheduling}]
	{
		\epsfig{file=figures/5FlowsHistoryTDF.eps, height=1.8in, width=3.3in}
		\label{Fig-HistoryTDF}
	}
\caption{\textbf{Experimental Results: Adaptive Virtual Time Scheduling Evaluation}}
\label{Fig-Adaptive}
\end{figure*}

\begin{table*}
\centering
\caption{\textbf{Comparison of Emulation Execution Time}}
\begin{tabular}{|c|c|c|c|c|} 
\hline
 & No Virtual Time & Adaptive Virtual Time & Fixed Virtual Time \\ 
\hline
Running Time (s)  & 240.730 & 1332.242 & 2434.910 \\ 
\hline
Average TDF & 1.000 & 5.900 & 11.000 \\ 
\hline
%Slow Down Ratio & 1.204 & 5.534 & 10.115 \\
Slow Down Ratio & 1.000 & 5.534 & 10.115 \\
%\hline
%AVG. CPU\% & 31.968 & 28.287 & 20.376 \\ 
%\hline
%STDEV. CPU\% & 10.915 & 7.175 & 4.173 \\ 
\hline
\end{tabular}
\label{Tab-CompareRunTime}
\end{table*}



%\subsection{System Overhead}
\paragraphb{System Overhead.}
Our virtual time system introduces overhead with the following two reasons: (1) the computation cost in Algorithm \ref{Alg-DilateTimeKeeping} and (2) the pauses of emulation when changing containers' TDFs. We measured both types of overhead and report the results in Table \ref{Tab-Overhead}.

First, we invoked both non-dilated and dilated \texttt{gettimeofday} 10,000,000 times from a user space application. The average overhead for one dilated \texttt{gettimeofday} is 0.013 microseconds.  We then used \texttt{strace} to count the number of invocations for \texttt{gettimeofday} in a 60-second \texttt{iperf3} run on both the server and the client. The total overhead is 18,145 microseconds after tracing 1,397,829 calls, which is about 0.03\% of the 60-second experiment. 
Actually, \texttt{iperf3} intensively invokes \texttt{gettimeofday}, because its timer is designed to exhaustively inquiry the OS time. The overhead amount will be even less for many other network applications. We also repeatedly changed a process's TDF 10,000,000 times using another test program. The average pause time was 0.063 microseconds, which is reasonably small. Since the number of TDF changes issued by the current adaptive TDF scheduling algorithm is a few orders of magnitude less than the number of calls to \texttt{gettimeofday} (e.g., only 14 TDF transitions occurred per host over the period of 1,332 seconds in the earlier adaptive TDF experiment), that overhead is also negligible.

\section{Case Study: Evaluation of ECMP in Data Center Networks}
\label{Sec-CaseStudy}
Network emulation testbeds are widely used to test and evaluate designs of network applications and protocols with the goal of discovering design limitations and implementation faults before the real system deployment. In this section, we present a case study to demonstrate how our virtual-time-enabled Mininet has been utilized to reproduce and validate the limitations of the equal-cost multi-path (ECMP) routing strategy in a data center network.

Many modern data center networks employ multi-rooted hierarchical tree topologies, and therefore ECMP-based protocols \cite{ECMP} are commonly used in data center networks for load-balancing traffic over multiple paths. When an ECMP-enabled switch has multiple next-hops on the best paths to a single destination, it selects the forwarding path by performing a modulo-N hash over the selected fields of a packet's header to ensure per-flow consistency. The key limitation of ECMP is that the communication bottleneck would occur when several large and long-lived flows collide on their hash and being forwarded to the same output port \cite{Hedera}. 
We borrowed the experiment scenario on a physical testbed described in \cite{Hedera}, and created a set of emulation experiments in Mininet to demonstrate the limitation of ECMP. We built a fat-tree topology in Mininet as shown in Figure \ref{Fig-FattreeTopoExample}, and generated stride-style traffic patterns. Note that stride($i$) means that a host with index $x$ sends to the host with index $(x + i)$ mod $n$, where $n$ is the number of hosts \cite{Hedera}. 
The hash-based ECMP mechanism is provided by the RipL-POX SDN controller \cite{RipLPox}. The Mininet code was developed with reference to \cite{ReproNetReserch}. In all the following experiments, we set up 8 sender-receiver pairs transmitting stride-pattern traffic flows using step 1 and 4. Figure \ref{Fig-FattreeTopoExampleStride1} shows the worst-case collision of 2 flows, distinguished by color, when stride step is 1; while in Figure \ref{Fig-FattreeTopoExampleStride4}, we see that 2 TCP flows will cause a lot of collision between core layer and aggregate layer, which is very common case.


\begin{figure*}[htbp]
\centering
\epsfig{file=figures/TopoFatTreeExample.eps, height=1.4in, width=4.8in}
\caption{\textbf{A Data Center Network with a Degree-4 Fat Tree Topology}}
\label{Fig-FattreeTopoExample}
\end{figure*}

\begin{figure*}[htbp]
\centering
\epsfig{file=figures/TopoFatTreeExampleStride1.eps, height=1.4in, width=4.8in}
\caption{\textbf{Worst-case TCP Flows with Stride Step = 1}}
\label{Fig-FattreeTopoExampleStride1}
\end{figure*}

\begin{figure*}[htbp]
\centering
\epsfig{file=figures/TopoFatTreeExampleStride4.eps, height=1.4in, width=4.8in}
\caption{\textbf{Common case TCP Flows with Stride Step = 4}}
\label{Fig-FattreeTopoExampleStride4}
\end{figure*}

We first set all the link bandwidth (switch-to-switch and switch-to-host) to 100 Mbps, and conducted each experiment over three independent runs. The average throughput of 8 TCP flows was plotted in Figure \ref{Fig-FatTreeAvgBw100M}, and each individual flow's throughput (24 in total) was plotted in Figure \ref{Fig-FatTreeIndividualBw100M}. The limitation of ECMP presented in \cite{Hedera} was clearly observed. When many conflicting flows occurred with stride-4 flow patterns, the average throughput in the fat-tree network dramatically fell below 30 Mbps with up to 75\% throughput drop. As shown in Figure \ref{Fig-FatTreeIndividualBw100M}, every flow's throughput was largely affected by the hash collision limitation of ECMP in the stride-4 scenario.

\begin{figure*}[htbp]
\centering
	\subfloat[\textbf{Average TCP Flow Throughput}]{
		\epsfig{file=figures/FattreeAvgAggBw100M.eps, height=1.8in, width=3.5in}
		\label{Fig-FatTreeAvgBw100M}
	}~
	\subfloat[\textbf{Throughput of Individual TCP Flow}]{
		\epsfig{file=figures/FattreeFlowDistBw100M.eps, height=1.8in, width=3.5in}
		\label{Fig-FatTreeIndividualBw100M}
	}
\caption{\textbf{Mininet Emulation Results: ECMP Limitation in a Fat-tree-based Data Center Network with 100 Mbps Link Bandwidth}}
\end{figure*}

However, the link bandwidth configuration in the previous experiments are not realistic. As early as in 2009, links connecting edge hosts to top of rack switches (ToR), ToR to edge of rank switches (EoR), and EoR to Core switches in a data center had been already above gigabit, in particular, 10 Gbps switch-to-switch links and 1 Gbps host-to-switch links \cite{ScaleEffDCN}. Can Mininet still show us the limitation of ECMP with such high link bandwidth? If not, can virtual time help to overcome the issue? Using the same configurations except that links were set to 10 Gbps, we re-ran the experiments in Mininet without virtual time ($TDF=1$) and with virtual time ($TDF = 4$). We plotted the average flow throughput in Figure \ref{Fig-FatTreeAvgBw10G} and individual flow throughput in Figure \ref{Fig-FatTreeIndividualBw10G}.

\begin{figure*}[htbp]
\centering

	\subfloat[\textbf{Average TCP Flow Throughput}]{
		\epsfig{file=figures/FattreeAvgAggBw10G.eps, height=2.1in, width=3.5in}
		\label{Fig-FatTreeAvgBw10G}
	}~
	\subfloat[\textbf{Throughput of Individual TCP Flow}]{
		\epsfig{file=figures/FattreeFlowDistBw10G.eps, height=2.1in, width=3.5in}
		\label{Fig-FatTreeIndividualBw10G}
	}

%\caption{Demonstration of How Virtual Time Helps Find Limitation of ECMP in DCN with Fat Tree Topology: All link bandwidth are 10 Gbps}
\caption{\textbf{Mininet Emulation Results with Virtual Time: ECMP Limitation in a Fat-tree-based Data Center Network with 10 Gbps Link Bandwidth}}
\end{figure*}

\begin{table*}
\centering
\caption{\textbf{Lightweight Virtual Time System: Overhead of System Calls}}
\begin{tabular}{|c|c|c|c|c|} 
\hline
 & No Virtual Time & Virtual Time & Avg Overhead Per System Call \\%& Overhead Rate in E  \\ 
\hline
\texttt{gettimeofday}  & 0.0532 $\mu$s & 0.0661 $\mu$s & 0.0129 $\mu$s\\% & $3.0 \times 10^{-4}$\\ 
\hline
\texttt{settimedilationfactor} & 0  & 0.0628 $\mu$s & 0.0628 $\mu$s\\% & $3.65\times 10^{-9}$ \\ 
\hline
\end{tabular}
\label{Tab-Overhead}
\end{table*}


In the case of stride-1, there were very few collisions among flows. Hence, the network performance ought to be close to the ideal throughput, i.e., 160 Gbps bisection bandwidth and 10 Gbps average bandwidth between each pair. In the experiments that $TDF=4$, the average throughput is above 9.0 Gbps, which is close to the theoretical value, and also match well with the results obtained from the physical testbed built upon 37 machines \cite{Hedera}. In the experiments that $TDF=1$, however, the throughput barely reaches 3.8 Gbps because of the limited system resources that Mininet can utilize. In addition, as shown in Figure \ref{Fig-FatTreeIndividualBw10G}, we observe that the variation of throughput is large among flows when $TDF=1$. This is incorrect because no flow shares the same link in the case of stride-1. In contrast, when $TDF = 4$, the throughput of all 8 flows are close with little variation, which implies the desired networking behaviors. 

In the case of stride-4, flows may collide both on the upstream and the downstream paths, thus using ECMP could undergo a significant throughput drop, e.g., up to 61\% as experimentally evaluated in \cite{Hedera}.  The virtual-time-enabled Mininet ($TDF=4$) has successfully captured such throughput drop phenomenon. We can see that average throughput dropped about 80\% when RipL-Pox controller used ECMP to handle multi-path flows. Large deviation (more than 55\% of average throughput value) also indicates that the flows were not properly scheduled with ECMP. When $TDF=1$ (no virtual time), Mininet also reported plummeted TCP throughput in the case of stride-4. However, we cannot use the result to experimentally demonstrate the limitation of ECMP. It is hard to distinguish whether the throughput drop was caused by insufficient resources to handle 10 Gbps or the limitation of ECMP, given the fact that the throughput was already too low in the collision-free case. Without a correct baseline (benchmark for the collision-free scenario), it is difficult to perform further analysis and qualify the limitation.

% In the first place, Mininet-Hifi without virtual time cannot guarantee a good precondition: when nearly no flow collisions exist at first, the throughput performance of DCN is already abnormally low. On the other hand, even though we assume the reduced throughput when $stride =4$ is actually caused by ECMP, we cannot say too much about how serious it can be: without a correct baseline, it is impossible to estimate the percentage of throughput loss.  At last, one may argue that the reason of bandwidth loss in Mininet-Hifi without virtual time is mainly because the emulator is unable to handle too much intervening 10Gbps flows, instead of the flaw of the ECMP routing mechanism.


